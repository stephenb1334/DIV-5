{
  "title": "Scaling Atlas for Large Datasets",
  "description": "A comprehensive guide to optimizing Nomic Atlas for handling large datasets efficiently.",
  "overview": {
    "purpose": "To help users configure and optimize Nomic Atlas for managing and visualizing large-scale datasets.",
    "use_cases": [
      "Uploading and processing datasets with millions of entries.",
      "Optimizing map rendering for high-dimensional data.",
      "Ensuring performance and reliability when working with extensive metadata."
    ]
  },
  "key_scaling_strategies": [
    {
      "strategy": "Data Preprocessing",
      "description": "Prepare datasets before uploading to Atlas to reduce complexity and improve processing times.",
      "recommendations": [
        "Normalize and clean your data to remove unnecessary fields or invalid entries.",
        "Use dimensionality reduction techniques (e.g., PCA) to reduce the size of embeddings.",
        "Split datasets into smaller chunks if they exceed size limits."
      ],
      "example_code": {
        "python": "import pandas as pd\nfrom sklearn.decomposition import PCA\n\n# Step 1: Load your dataset\ndata = pd.read_csv('large_dataset.csv')\n\n# Step 2: Apply PCA for dimensionality reduction\npca = PCA(n_components=50)\nreduced_embeddings = pca.fit_transform(data[['embedding1', 'embedding2', 'embedding3']])\ndata['reduced_embedding'] = list(reduced_embeddings)\n\n# Step 3: Save the preprocessed dataset\ndata.to_csv('preprocessed_dataset.csv', index=False)"
      }
    },
    {
      "strategy": "Chunked Uploads",
      "description": "Upload large datasets in smaller chunks to avoid hitting API limits.",
      "recommendations": [
        "Divide datasets into chunks of 100,000 rows or less.",
        "Use the `/v1/data/upload` API endpoint iteratively for each chunk."
      ],
      "example_code": {
        "python": "import pandas as pd\nimport requests\n\n# Step 1: Load your dataset\ndata = pd.read_csv('large_dataset.csv')\n\n# Step 2: Split into chunks\nchunk_size = 100000\nchunks = [data[i:i+chunk_size] for i in range(0, data.shape[0], chunk_size)]\n\n# Step 3: Upload each chunk\nfor i, chunk in enumerate(chunks):\n    chunk_file = f'chunk_{i}.csv'\n    chunk.to_csv(chunk_file, index=False)\n    with open(chunk_file, 'rb') as f:\n        response = requests.post(\n            'https://api-atlas.nomic.ai/v1/data/upload',\n            headers={'Authorization': 'Bearer <API_KEY>'},\n            files={'file': f}\n        )\n        print(f'Chunk {i} upload response:', response.json())"
      }
    },
    {
      "strategy": "Efficient Map Rendering",
      "description": "Optimize map rendering for large datasets by adjusting visualization settings.",
      "recommendations": [
        "Use UMAP instead of t-SNE for faster dimensionality reduction.",
        "Enable clustering to group similar data points and reduce visual clutter.",
        "Limit the number of visible data points for initial renders."
      ],
      "example_api_request": {
        "curl": "curl -X POST 'https://api-atlas.nomic.ai/v1/maps/settings' -H 'Authorization: Bearer <API_KEY>' -H 'Content-Type: application/json' -d '{\"settings\": {\"dimensionality_reduction\": \"UMAP\", \"clustering\": true, \"max_points\": 10000}}'"
      }
    },
    {
      "strategy": "Metadata Optimization",
      "description": "Streamline metadata to improve performance and reduce API payload sizes.",
      "recommendations": [
        "Remove redundant or irrelevant metadata fields.",
        "Use concise field names to minimize payload size.",
        "Compress metadata fields where possible (e.g., use codes instead of full strings)."
      ],
      "example": {
        "before": {
          "metadata": {
            "author_name": "John Doe",
            "publication_date": "2023-06-15",
            "category": "Technology"
          }
        },
        "after": {
          "metadata": {
            "auth": "JD",
            "pub_date": "2023-06-15",
            "cat": "Tech"
          }
        }
      }
    },
    {
      "strategy": "Monitoring and Scaling Resources",
      "description": "Monitor performance and scale resources as needed for large datasets.",
      "recommendations": [
        "Enable API usage logs to track performance bottlenecks.",
        "Upgrade your API tier or request increased limits for large-scale workloads.",
        "Use cloud-based storage solutions like AWS S3 for efficient data management."
      ],
      "example_api_request": {
        "curl": "curl -X GET 'https://api-atlas.nomic.ai/v1/logs' -H 'Authorization: Bearer <API_KEY>'"
      }
    }
  ],
  "implementation_steps": {
    "steps": [
      {
        "step": 1,
        "title": "Preprocess the Dataset",
        "description": "Clean and normalize your dataset, apply dimensionality reduction, and split large files into manageable chunks."
      },
      {
        "step": 2,
        "title": "Upload Data in Chunks",
        "description": "Use the `/v1/data/upload` API endpoint to upload each chunk iteratively. Monitor progress to ensure all chunks are uploaded successfully."
      },
      {
        "step": 3,
        "title": "Configure Visualization Settings",
        "description": "Adjust settings like dimensionality reduction method, clustering, and max visible points for optimized map rendering."
      },
      {
        "step": 4,
        "title": "Monitor Performance",
        "description": "Enable API logs and track usage to identify bottlenecks. Scale resources or adjust settings as necessary."
      }
    ]
  },
  "usage_examples": {
    "python_script": {
      "description": "Automate chunked uploads and optimized map settings using Python.",
      "code": "import pandas as pd\nimport requests\n\n# Step 1: Load and preprocess the dataset\ndata = pd.read_csv('large_dataset.csv')\ndata = data.dropna()  # Remove null values\n\n# Step 2: Split into chunks\nchunk_size = 100000\nchunks = [data[i:i+chunk_size] for i in range(0, data.shape[0], chunk_size)]\n\n# Step 3: Upload chunks and configure settings\nfor i, chunk in enumerate(chunks):\n    chunk_file = f'chunk_{i}.csv'\n    chunk.to_csv(chunk_file, index=False)\n    with open(chunk_file, 'rb') as f:\n        response = requests.post(\n            'https://api-atlas.nomic.ai/v1/data/upload',\n            headers={'Authorization': 'Bearer <API_KEY>'},\n            files={'file': f}\n        )\n        print(f'Uploaded chunk {i}:', response.json())\n\n# Step 4: Configure map settings\nsettings_url = 'https://api-atlas.nomic.ai/v1/maps/settings'\nsettings_payload = {\n    'settings': {\n        'dimensionality_reduction': 'UMAP',\n        'clustering': True,\n        'max_points': 10000\n    }\n}\nresponse = requests.post(settings_url, headers={'Authorization': 'Bearer <API_KEY>'}, json=settings_payload)\nprint('Map settings response:', response.json())"
    }
  },
  "notes": [
    "Large datasets may require additional preprocessing time, especially for embedding generation or dimensionality reduction.",
    "API rate limits can impact upload speeds; consider upgrading your API tier if necessary.",
    "Refer to the Nomic Atlas API documentation for detailed information on scaling and performance optimization."
  ]
}