{
  "title": "Atlas API Rate Limits and Optimization",
  "description": "A comprehensive guide to understanding Atlas API rate limits and implementing optimization strategies for efficient workflows.",
  "overview": {
    "purpose": "To help users manage API rate limits effectively and optimize API usage for scalable and reliable workflows.",
    "use_cases": [
      "Uploading large datasets without exceeding API limits.",
      "Implementing retries and error handling for robust workflows.",
      "Optimizing API calls to improve performance and reduce costs."
    ]
  },
  "understanding_rate_limits": {
    "description": "Atlas API enforces rate limits to ensure fair usage and maintain system stability.",
    "key_points": [
      "Rate limits are applied per API key and vary based on the API tier (e.g., Free, Pro, Enterprise).",
      "Exceeding rate limits results in HTTP 429 (Too Many Requests) responses.",
      "Rate limits reset periodically, depending on your plan (e.g., every minute or hour)."
    ],
    "example_response": {
      "status_code": 429,
      "message": "Too Many Requests",
      "retry_after": 60
    }
  },
  "optimization_strategies": [
    {
      "strategy": "Batch API Calls",
      "description": "Combine multiple requests into a single batch to reduce the number of API calls.",
      "recommendations": [
        "Use the batch upload feature for datasets.",
        "Group related operations into a single request where supported."
      ],
      "example_code": {
        "python": "import requests\n\n# Batch upload example\nurl = 'https://api-atlas.nomic.ai/v1/data/upload'\nheaders = {'Authorization': 'Bearer <API_KEY>', 'Content-Type': 'application/json'}\ndata_batch = [\n    {'text': 'example1', 'metadata': {'category': 'A'}},\n    {'text': 'example2', 'metadata': {'category': 'B'}}\n]\nresponse = requests.post(url, headers=headers, json={'data': data_batch})\nprint(response.json())"
      }
    },
    {
      "strategy": "Implement Exponential Backoff",
      "description": "Use exponential backoff to handle rate limit errors and retry requests after a delay.",
      "recommendations": [
        "Start with a small delay (e.g., 1 second) and double it for each subsequent retry.",
        "Limit the maximum number of retries to avoid infinite loops."
      ],
      "example_code": {
        "python": "import time\nimport requests\n\n# Exponential backoff example\nurl = 'https://api-atlas.nomic.ai/v1/data/upload'\nheaders = {'Authorization': 'Bearer <API_KEY>', 'Content-Type': 'application/json'}\ndata = {'text': 'example', 'metadata': {'category': 'A'}}\n\nmax_retries = 5\nretry_delay = 1\n\nfor attempt in range(max_retries):\n    response = requests.post(url, headers=headers, json={'data': [data]})\n    if response.status_code == 429:\n        retry_after = int(response.headers.get('Retry-After', retry_delay))\n        print(f'Rate limit exceeded. Retrying in {retry_after} seconds...')\n        time.sleep(retry_after)\n        retry_delay *= 2\n    else:\n        print(response.json())\n        break"
      }
    },
    {
      "strategy": "Optimize Data Uploads",
      "description": "Reduce the size and frequency of uploads to avoid hitting rate limits.",
      "recommendations": [
        "Preprocess and compress data before uploading.",
        "Split large datasets into smaller chunks and upload them incrementally.",
        "Avoid redundant uploads by checking for existing data."
      ],
      "example_code": {
        "python": "import pandas as pd\nimport requests\n\n# Preprocess and split data into chunks\nlarge_dataset = pd.read_csv('large_dataset.csv')\nchunk_size = 10000\nchunks = [large_dataset[i:i+chunk_size] for i in range(0, len(large_dataset), chunk_size)]\n\n# Upload chunks incrementally\nurl = 'https://api-atlas.nomic.ai/v1/data/upload'\nheaders = {'Authorization': 'Bearer <API_KEY}', 'Content-Type': 'application/json'}\n\nfor i, chunk in enumerate(chunks):\n    chunk_data = chunk.to_dict(orient='records')\n    response = requests.post(url, headers=headers, json={'data': chunk_data})\n    print(f'Chunk {i} upload response:', response.json())"
      }
    },
    {
      "strategy": "Monitor API Usage",
      "description": "Track API usage and identify patterns to optimize workflows.",
      "recommendations": [
        "Enable API usage logs and monitor request counts.",
        "Identify high-frequency endpoints and optimize their usage.",
        "Upgrade to a higher API tier if needed."
      ],
      "example_api_request": {
        "curl": "curl -X GET 'https://api-atlas.nomic.ai/v1/logs' -H 'Authorization: Bearer <API_KEY>'"
      }
    },
    {
      "strategy": "Use Asynchronous Requests",
      "description": "Send asynchronous requests to improve performance and avoid blocking workflows.",
      "recommendations": [
        "Leverage Python's `asyncio` library or similar frameworks.",
        "Use asynchronous requests for batch uploads and large-scale operations."
      ],
      "example_code": {
        "python": "import asyncio\nimport aiohttp\n\nasync def upload_data(session, url, headers, data):\n    async with session.post(url, headers=headers, json={'data': data}) as response:\n        return await response.json()\n\nasync def main():\n    url = 'https://api-atlas.nomic.ai/v1/data/upload'\n    headers = {'Authorization': 'Bearer <API_KEY>', 'Content-Type': 'application/json'}\n    data_batches = [\n        [{'text': 'example1', 'metadata': {'category': 'A'}}],\n        [{'text': 'example2', 'metadata': {'category': 'B'}}]\n    ]\n\n    async with aiohttp.ClientSession() as session:\n        tasks = [upload_data(session, url, headers, batch) for batch in data_batches]\n        responses = await asyncio.gather(*tasks)\n        print(responses)\n\nasyncio.run(main())"
      }
    }
  ],
  "implementation_steps": {
    "steps": [
      {
        "step": 1,
        "title": "Understand Your API Tier",
        "description": "Review the rate limits and capabilities of your current API plan."
      },
      {
        "step": 2,
        "title": "Analyze API Usage Patterns",
        "description": "Identify high-usage endpoints and optimize their workflows."
      },
      {
        "step": 3,
        "title": "Implement Optimization Strategies",
        "description": "Apply techniques such as batching, backoff, and asynchronous requests to reduce API load."
      },
      {
        "step": 4,
        "title": "Monitor and Adjust",
        "description": "Track API usage regularly and refine workflows to ensure efficient utilization."
      }
    ]
  },
  "usage_examples": {
    "python_script": {
      "description": "Handle rate limits and optimize API calls for uploading large datasets.",
      "code": "import time\nimport requests\n\n# Step 1: Define the API endpoint and headers\nurl = 'https://api-atlas.nomic.ai/v1/data/upload'\nheaders = {'Authorization': 'Bearer <API_KEY>', 'Content-Type': 'application/json'}\n\n# Step 2: Prepare data in batches\ndata_batches = [\n    [{'text': 'example1', 'metadata': {'category': 'A'}}],\n    [{'text': 'example2', 'metadata': {'category': 'B'}}]\n]\n\n# Step 3: Upload data with exponential backoff\nfor batch in data_batches:\n    retry_delay = 1\n    while True:\n        response = requests.post(url, headers=headers, json={'data': batch})\n        if response.status_code == 429:\n            retry_after = int(response.headers.get('Retry-After', retry_delay))\n            print(f'Rate limit exceeded. Retrying in {retry_after} seconds...')\n            time.sleep(retry_after)\n            retry_delay *= 2\n        else:\n            print(response.json())\n            break"
    }
  },
  "notes": [
    "Efficient API usage ensures workflows remain scalable and cost-effective.",
    "Regularly review API usage logs to identify opportunities for optimization.",
    "Refer to the Atlas API documentation for the latest rate limit policies and optimization techniques."
  ]
}